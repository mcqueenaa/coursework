{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program makes valence models for the relevant verbs on Geometry.\n",
    "Verbs were collected from the 'getting verbs' program.\n",
    "Valence models are saved into txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2 as pm2 \n",
    "pmm = pm2.MorphAnalyzer() \n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sony\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"only relevant verbs.json\", \"r\", encoding = 'utf-8') as t:\n",
    "    good_verbs = json.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text): ##getting simple sentences from text\n",
    "    good_sentences = []\n",
    "    sentences = []\n",
    "    regSubS = re.compile('[А-Яа-я ]+')\n",
    "    for line in text:\n",
    "        if line != ' \\n':\n",
    "            curr_sents = list(split_sentences(line))\n",
    "            for i in curr_sents:\n",
    "                sentences.append(i)\n",
    "                \n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 10:\n",
    "            for subsent in re.findall(regSubS, sentence):\n",
    "                if len(subsent.split()) > 1:\n",
    "                    good_sentences.append(subsent)\n",
    "    return good_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting good sentences\n",
    "with open('all geometry.txt', 'r', encoding='utf-8') as t: \n",
    "    text = t.readlines()\n",
    "\n",
    "good_sentences = get_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88766"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemmas(sent): ## turning sentence into the list of its lemmas\n",
    "    sent = re.sub(r'[^\\w\\s]','', sent) \n",
    "    sent = re.sub(r'\\d', '', sent) \n",
    "    sent = re.sub(r'[A-Za-z]', '', sent)\n",
    "    lem_sent = [pmm.normal_forms(x)[0] for x in sent.split()]\n",
    "    return lem_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_contexts(): ## getting a dict of the verbs with their contexts\n",
    "    contexts = {}\n",
    "    for verb in good_verbs:\n",
    "        if verb != 'объесть':\n",
    "            contexts[verb] = []\n",
    "    for sent in good_sentences:\n",
    "        for verb in good_verbs:\n",
    "            if verb != 'объесть':\n",
    "                lem_sent = get_lemmas(sent)\n",
    "                if verb in lem_sent:\n",
    "                    contexts[verb].append(sent)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts = get_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_json(title, variable): ## saving a variable into json a file. title - name of the json file\n",
    "    with open(title, \"w\", encoding = 'utf-8') as t:\n",
    "        json.dump(variable, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_json(title): ## getting a variable from a json file. title - name of the json file\n",
    "    with open(title, \"r\", encoding = 'utf-8') as t:\n",
    "        variable = json.load(t)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## saving verbs and their contexts into json file \n",
    "save_to_json(\"contexts.json\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting verbs and their contexts from the json file\n",
    "contexts = get_from_json(\"contexts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_current_words_for_trans_v(sent): ## getting transitive verbs and their subjects and objects from sentence\n",
    "    curr_words = []\n",
    "    ex_pos = None\n",
    "    prtf_gr = None\n",
    "    for word in sent.split():\n",
    "        nomn = False\n",
    "        w_gr = str(pmm.tag(word)[0])\n",
    "        pos = w_gr.split()[0].split(',')[0]\n",
    "        if prtf_gr == None:\n",
    "            if pos == 'PREP':\n",
    "                curr_words.append((pmm.normal_forms(word)[0], pos))\n",
    "            elif pos == 'NOUN':\n",
    "                case = w_gr.split()[1].split(',')[1]\n",
    "                if case == 'gent': \n",
    "                    if ex_pos == 'PREP':\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                else:   \n",
    "                    if case == 'nomn':\n",
    "                        if nomn == False:\n",
    "                            nomn = True\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                        else:\n",
    "                            case = 'accs'\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                    else:\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case))    \n",
    "            elif pos == 'PRTF':\n",
    "                prtf_gr = w_gr.split()[1]\n",
    "        else:\n",
    "            if pos == 'NOUN':\n",
    "                if len(prtf_gr.split(',')) == 3:\n",
    "                    noun_gr = w_gr.split()[0].split(',')[2] + ',' + w_gr.split()[1]\n",
    "                else:\n",
    "                    noun_gr = w_gr.split()[1]\n",
    "                if noun_gr == prtf_gr:\n",
    "                    prtf_gr = None\n",
    "                    case = w_gr.split()[1].split(',')[1]\n",
    "                    if case == 'gent': \n",
    "                        if ex_pos == 'PREP':\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                    else:   \n",
    "                        if case == 'nomn':\n",
    "                            if nomn == False:\n",
    "                                nomn = True\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                            else:\n",
    "                                case = 'accs'\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                        else:\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "        if pos != 'ADJF' and pos != 'PRTF':\n",
    "            ex_pos = pos\n",
    "        else:\n",
    "            ex_pos = ex_pos\n",
    "    return curr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_current_words_for_intr_v(sent): ## getting intransitive verbs and their subjects and objects from sentence\n",
    "    curr_words = []\n",
    "    ex_pos = None\n",
    "    prtf_gr = None\n",
    "    for word in sent.split():\n",
    "        w_gr = str(pmm.tag(word)[0])\n",
    "        pos = w_gr.split()[0].split(',')[0]\n",
    "        if prtf_gr == None:\n",
    "            if pos == 'PREP':\n",
    "                curr_words.append((pmm.normal_forms(word)[0], pos))\n",
    "            elif pos == 'NOUN':            \n",
    "                case = w_gr.split()[1].split(',')[1]\n",
    "                if case != 'accs':\n",
    "                    if case == 'gent':\n",
    "                        if ex_pos == 'PREP':\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "                    else:\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "            elif pos == 'PRTF':\n",
    "                prtf_gr = w_gr.split()[1]               \n",
    "        else:\n",
    "            if pos == 'NOUN':\n",
    "                if len(prtf_gr.split(',')) == 3:\n",
    "                    noun_gr = w_gr.split()[0].split(',')[2] + ',' + w_gr.split()[1]\n",
    "                else:\n",
    "                    noun_gr = w_gr.split()[1]\n",
    "                if noun_gr == prtf_gr:\n",
    "                    prtf_gr = None \n",
    "                    case = w_gr.split()[1].split(',')[1]\n",
    "                    if case != 'accs':\n",
    "                        if case == 'gent':  \n",
    "                            if ex_pos == 'PREP':\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "                        else:\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "        if pos != 'ADJF' and pos != 'PRTF':\n",
    "            ex_pos = pos\n",
    "        else:\n",
    "            ex_pos = ex_pos\n",
    "    return curr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_valencies(contexts): ## getting all verbs and their subjects and objects for the whole text\n",
    "    model = {}\n",
    "    for key, value in contexts.items():\n",
    "        curr_model = []\n",
    "        v_gr = str(pmm.tag(key)[0])\n",
    "        is_tran = v_gr.split()[0].split(',')[2]\n",
    "        if is_tran == 'tran':\n",
    "            for sent in value:\n",
    "                curr_words = get_current_words_for_trans_v(sent)\n",
    "                curr_model.append(curr_words)\n",
    "        elif is_tran == 'intr':\n",
    "            for sent in value:\n",
    "                curr_words = get_current_words_for_intr_v(sent)\n",
    "                curr_model.append(curr_words)\n",
    "        model[key] = curr_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getting_computed_valencies(contexts): \n",
    "    ## collects all verbs and their valencies with number of uses in text into one great dict of dicts\n",
    "    model = get_valencies(contexts)\n",
    "    computed_models = {}\n",
    "    for key, value in model.items():\n",
    "        verb_model = {}\n",
    "        for sent in range(len(value)):\n",
    "            for idx, word in enumerate(value[sent]):\n",
    "                if word[1] == 'PREP':\n",
    "                    if idx < (len(value[sent]) - 1):\n",
    "                        if value[sent][idx+1][1] == 'NOUN':\n",
    "                            n_info = value[sent][idx+1]\n",
    "                            if n_info[2] == 'nomn':\n",
    "                                n_info[2] = 'accs'\n",
    "                            if word[0] not in verb_model:\n",
    "                                verb_model[word[0] + ' + ' + n_info[2]] = {n_info[0] : 1}\n",
    "                            else:\n",
    "                                if n_info[0] in verb_model[word[0] + ' + ' + n_info[2]]:\n",
    "                                    verb_model[word[0] + ' + ' + n_info[2]][n_info[0]] += 1\n",
    "                                else:\n",
    "                                    verb_model[word[0] + ' + ' + n_info[2]][n_info[0]] = 1\n",
    "                if word[1] == 'NOUN':\n",
    "                    if idx != 0:\n",
    "                        if value[sent][idx-1][1] != 'PREP':\n",
    "                            if word[2] == 'nomn':\n",
    "                                if 'подлежащее, nomn' not in verb_model:\n",
    "                                    verb_model['подлежащее, nomn'] = {word[0] : 1}\n",
    "                                else:\n",
    "                                    if word[0] in verb_model['подлежащее, nomn']:\n",
    "                                        verb_model['подлежащее, nomn'][word[0]] += 1\n",
    "                                    else:\n",
    "                                        verb_model['подлежащее, nomn'][word[0]] = 1\n",
    "                            else:\n",
    "                                v_gr = str(pmm.tag(key)[0])\n",
    "                                is_tran = v_gr.split()[0].split(',')[2]\n",
    "                                if is_tran == 'tran':\n",
    "                                    if word[2] == 'accs': \n",
    "                                        if 'прямое дополнение, accs' not in verb_model:\n",
    "                                            verb_model['прямое дополнение, accs'] = {word[0] : 1}\n",
    "                                        else:\n",
    "                                            if word[0] in verb_model['прямое дополнение, accs']:\n",
    "                                                verb_model['прямое дополнение, accs'][word[0]] += 1\n",
    "                                            else:\n",
    "                                                verb_model['прямое дополнение, accs'][word[0]] = 1\n",
    "                                    else: \n",
    "                                        title = 'непрямое дополнение, ' + word[2]\n",
    "                                        if title not in verb_model:\n",
    "                                            verb_model[title] = {word[0] : 1}\n",
    "                                        else:\n",
    "                                            if word[0] in verb_model[title]:\n",
    "                                                verb_model[title][word[0]] += 1\n",
    "                                            else:\n",
    "                                                verb_model[title][word[0]] = 1\n",
    "                                else: \n",
    "                                    title = 'непрямое дополнение, ' + word[2]\n",
    "                                    if title not in verb_model:\n",
    "                                        verb_model[title] = {word[0] : 1}\n",
    "                                    else:\n",
    "                                        if word[0] in verb_model[title]:\n",
    "                                            verb_model[title][word[0]] += 1\n",
    "                                        else:\n",
    "                                            verb_model[title][word[0]] = 1\n",
    "        computed_models[key] = verb_model\n",
    "        \n",
    "    return(computed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computed_models = getting_computed_valencies(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_to_json('computed models.json', computed_models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_models = get_from_json('computed models.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_dict_writer(path, fieldnames, data): ## writes a CSV file using DictWriter\n",
    "    with open(path, \"w\", newline='') as out_file:\n",
    "        writer = csv.DictWriter(out_file, delimiter=';', fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_valencies(computed_models):\n",
    "    for verb, parse in computed_models.items():\n",
    "        fieldnames = []\n",
    "        data = []\n",
    "        last_data = []\n",
    "        if parse:\n",
    "            for obj, exmpls in parse.items():\n",
    "                fieldnames.append(obj)\n",
    "                fieldnames.append('количество примеров для колонки \"' + obj + '\"')\n",
    "                \n",
    "                obj_data = []\n",
    "                numb_data = []\n",
    "                for word, numb in exmpls.items():\n",
    "                    obj_data.append(word)\n",
    "                    numb_data.append(str(numb))\n",
    "                data.append(obj_data)\n",
    "                data.append(numb_data)\n",
    "                elements = [] \n",
    "                for arr in data: \n",
    "                    if len(arr) > 1:\n",
    "                        element = '\\n'.join(arr)\n",
    "                    else:\n",
    "                        element = arr[0]\n",
    "                    elements.append(element)\n",
    "                inner_dict = dict(zip(fieldnames, elements))\n",
    "            last_data.append(inner_dict)\n",
    "        csv_dict_writer('valencies/csv/Модель управления глагола ' + verb + '.csv', fieldnames, last_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_valencies(computed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_save_valences(computed_models):\n",
    "    for verb, parse in computed_models.items():\n",
    "        if parse:\n",
    "            with open('valencies/txt/Модель управления глагола ' + verb + '.txt', 'a', encoding = 'utf-8') as v:\n",
    "                v.write('Модель управления глагола ' + verb + ':\\n')\n",
    "                for obj, exmpls in parse.items():\n",
    "                    v.write(obj + ': ' )\n",
    "                    for exmpl, value in exmpls.items():\n",
    "                        v.write(exmpl + ' = ' + str(value) + '; ')\n",
    "                    v.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_save_valences(computed_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
