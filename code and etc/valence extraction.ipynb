{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program makes valence models for the relevant verbs on Geometry.\n",
    "Verbs were collected from the 'getting verbs' program.\n",
    "Valence models are saved into txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2 as pm2 \n",
    "pmm = pm2.MorphAnalyzer() \n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sony\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"only relevant verbs.json\", \"r\", encoding = 'utf-8') as t:\n",
    "    good_verbs = json.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['служить',\n",
       " 'являться',\n",
       " 'обладать',\n",
       " 'иметь',\n",
       " 'определяться',\n",
       " 'означать',\n",
       " 'выражаться',\n",
       " 'состоять',\n",
       " 'лежать',\n",
       " 'принадлежать',\n",
       " 'существовать',\n",
       " 'называться',\n",
       " 'рассмотреть',\n",
       " 'найти',\n",
       " 'получаться',\n",
       " 'образовать',\n",
       " 'использоваться',\n",
       " 'находить',\n",
       " 'позволять',\n",
       " 'разбивать',\n",
       " 'делить',\n",
       " 'обозначаться',\n",
       " 'проходить',\n",
       " 'следовать',\n",
       " 'пересекаться',\n",
       " 'доказать',\n",
       " 'допустить',\n",
       " 'сформулировать',\n",
       " 'пересекать',\n",
       " 'построить',\n",
       " 'воспользоваться',\n",
       " 'совпадать',\n",
       " 'совместиться',\n",
       " 'совмещаться',\n",
       " 'выполняться',\n",
       " 'вычислить',\n",
       " 'содержимый',\n",
       " 'сравнить',\n",
       " 'отложить',\n",
       " 'получать',\n",
       " 'требоваться',\n",
       " 'вытекать',\n",
       " 'доказываться',\n",
       " 'обозначить',\n",
       " 'удовлетворять',\n",
       " 'начертить',\n",
       " 'выразить',\n",
       " 'объяснить',\n",
       " 'пересечь',\n",
       " 'вычисляться',\n",
       " 'сохранять',\n",
       " 'отображаться',\n",
       " 'объесть',\n",
       " 'переводить',\n",
       " 'переходить',\n",
       " 'проектироваться',\n",
       " 'нарисовать']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text): ##getting simple sentences from text\n",
    "    good_sentences = []\n",
    "    sentences = []\n",
    "    regSubS = re.compile('[А-Яа-я ]+')\n",
    "    for line in text:\n",
    "        if line != ' \\n':\n",
    "            curr_sents = list(split_sentences(line))\n",
    "            for i in curr_sents:\n",
    "                sentences.append(i)\n",
    "                \n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 10:\n",
    "            for subsent in re.findall(regSubS, sentence):\n",
    "                if len(subsent.split()) > 1:\n",
    "                    good_sentences.append(subsent)\n",
    "    return good_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting good sentences\n",
    "with open('all geometry.txt', 'r', encoding='utf-8') as t: \n",
    "    text = t.readlines()\n",
    "#len(text)\n",
    "\n",
    "good_sentences = get_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88766"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemmas(sent): ## turning sentence into the list of its lemmas\n",
    "    sent = re.sub(r'[^\\w\\s]','', sent) \n",
    "    sent = re.sub(r'\\d', '', sent) \n",
    "    sent = re.sub(r'[A-Za-z]', '', sent)\n",
    "    lem_sent = [pmm.normal_forms(x)[0] for x in sent.split()]\n",
    "    return lem_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_contexts(): ## getting a dict of the verbs with their contexts\n",
    "    contexts = {}\n",
    "    for verb in good_verbs:\n",
    "        contexts[verb] = []\n",
    "    for sent in good_sentences:\n",
    "        for verb in good_verbs:\n",
    "            lem_sent = get_lemmas(sent)\n",
    "            if verb in lem_sent:\n",
    "                contexts[verb].append(sent)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = get_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_json(title, variable): ## saving a variable into json a file. title - name of the json file\n",
    "    with open(title, \"w\", encoding = 'utf-8') as t:\n",
    "        json.dump(variable, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_json(title): ## getting a variable from a json file. title - name of the json file\n",
    "    with open(title, \"r\", encoding = 'utf-8') as t:\n",
    "        variable = json.load(t)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## saving verbs and their contexts into json file \n",
    "save_to_json(\"contexts.json\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting verbs and their contexts from the json file\n",
    "contexts = get_from_json(\"contexts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_words_for_trans_v(sent): ## getting transitive verbs and their subjects and objects from sentence\n",
    "    curr_words = []\n",
    "    ex_pos = None\n",
    "    prtf_gr = None\n",
    "    for word in sent.split():\n",
    "        nomn = False\n",
    "        w_gr = str(pmm.tag(word)[0])\n",
    "        pos = w_gr.split()[0].split(',')[0]\n",
    "        if prtf_gr == None:\n",
    "            if pos == 'PREP':\n",
    "                curr_words.append((pmm.normal_forms(word)[0], pos))\n",
    "            elif pos == 'NOUN':\n",
    "                case = w_gr.split()[1].split(',')[1]\n",
    "                if case == 'gent': \n",
    "                    if ex_pos == 'PREP':\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                else:   \n",
    "                    if case == 'nomn':\n",
    "                        if nomn == False:\n",
    "                            nomn = True\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                        else:\n",
    "                            case = 'accs'\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                    else:\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case))    \n",
    "            elif pos == 'PRTF':\n",
    "                prtf_gr = w_gr.split()[1]\n",
    "        else:\n",
    "            if pos == 'NOUN':\n",
    "                if len(prtf_gr.split(',')) == 3:\n",
    "                    noun_gr = w_gr.split()[0].split(',')[2] + ',' + w_gr.split()[1]\n",
    "                else:\n",
    "                    noun_gr = w_gr.split()[1]\n",
    "                if noun_gr == prtf_gr:\n",
    "                    prtf_gr = None\n",
    "                    case = w_gr.split()[1].split(',')[1]\n",
    "                    if case == 'gent': \n",
    "                        if ex_pos == 'PREP':\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                    else:   \n",
    "                        if case == 'nomn':\n",
    "                            if nomn == False:\n",
    "                                nomn = True\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                            else:\n",
    "                                case = 'accs'\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "                        else:\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "        if pos != 'ADJF' and pos != 'PRTF':\n",
    "            ex_pos = pos\n",
    "        else:\n",
    "            ex_pos = ex_pos\n",
    "    return curr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_words_for_intr_v(sent): ## getting intransitive verbs and their subjects and objects from sentence\n",
    "    curr_words = []\n",
    "    ex_pos = None\n",
    "    prtf_gr = None\n",
    "    for word in sent.split():\n",
    "        w_gr = str(pmm.tag(word)[0])\n",
    "        pos = w_gr.split()[0].split(',')[0]\n",
    "        if prtf_gr == None:\n",
    "            if pos == 'PREP':\n",
    "                curr_words.append((pmm.normal_forms(word)[0], pos))\n",
    "            elif pos == 'NOUN':            \n",
    "                case = w_gr.split()[1].split(',')[1]\n",
    "                if case != 'accs':\n",
    "                    if case == 'gent':\n",
    "                        if ex_pos == 'PREP':\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "                    else:\n",
    "                        curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "            elif pos == 'PRTF':\n",
    "                prtf_gr = w_gr.split()[1]               \n",
    "        else:\n",
    "            if pos == 'NOUN':\n",
    "                if len(prtf_gr.split(',')) == 3:\n",
    "                    noun_gr = w_gr.split()[0].split(',')[2] + ',' + w_gr.split()[1]\n",
    "                else:\n",
    "                    noun_gr = w_gr.split()[1]\n",
    "                if noun_gr == prtf_gr:\n",
    "                    prtf_gr = None \n",
    "                    case = w_gr.split()[1].split(',')[1]\n",
    "                    if case != 'accs':\n",
    "                        if case == 'gent':  \n",
    "                            if ex_pos == 'PREP':\n",
    "                                curr_words.append((pmm.normal_forms(word)[0], pos, case)) \n",
    "                        else:\n",
    "                            curr_words.append((pmm.normal_forms(word)[0], pos, case))\n",
    "        if pos != 'ADJF' and pos != 'PRTF':\n",
    "            ex_pos = pos\n",
    "        else:\n",
    "            ex_pos = ex_pos\n",
    "    return curr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('храм', 'NOUN', 'nomn'), ('подтверждение', 'NOUN', 'ablt')]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_words_for_intr_v('Сохранившиеся до наших времен и поражающие своим величием храмы и гробницы египетских фараонов служат убедительным подтверждением высокого уровня геометрических знаний древних египтян') ## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valences(contexts): ##getting all verbs and their subjects and objects for the whole text\n",
    "    model = {}\n",
    "    for key, value in contexts.items():\n",
    "        curr_model = []\n",
    "        v_gr = str(pmm.tag(key)[0])\n",
    "        is_tran = v_gr.split()[0].split(',')[2]\n",
    "        if is_tran == 'tran':\n",
    "            for sent in value:\n",
    "                curr_words = get_current_words_for_trans_v(sent)\n",
    "                curr_model.append(curr_words)\n",
    "        elif is_tran == 'intr':\n",
    "            for sent in value:\n",
    "                curr_words = get_current_words_for_intr_v(sent)\n",
    "                curr_model.append(curr_words)\n",
    "        #print(curr_model)\n",
    "        model[key] = curr_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'базироваться': [[('на', 'PREP'), ('мироздание', 'NOUN', 'nomn')]],\n",
       " 'владеть': [[('геометрия', 'NOUN', 'ablt')], []],\n",
       " 'вносить': [],\n",
       " 'вселять': [],\n",
       " 'выражаться': [[('язык', 'NOUN', 'ablt'), ('геометрия', 'NOUN', 'nomn')]],\n",
       " 'вычислять': [[('объём', 'NOUN', 'accs')]],\n",
       " 'доходить': [],\n",
       " 'измерять': [[]],\n",
       " 'изображать': [],\n",
       " 'изучаться': [[('в', 'PREP'),\n",
       "   ('планиметрия', 'NOUN', 'gent'),\n",
       "   ('в', 'PREP'),\n",
       "   ('основное', 'NOUN', 'loct')]],\n",
       " 'исчерпывать': [[]],\n",
       " 'лежать': [[('в', 'PREP'), ('плоскость', 'NOUN', 'gent')],\n",
       "  [('в', 'PREP'), ('плоскость', 'NOUN', 'gent')]],\n",
       " 'накоплять': [],\n",
       " 'начинать': [],\n",
       " 'обладать': [[('привлекательность', 'NOUN', 'ablt')]],\n",
       " 'обусловливать': [],\n",
       " 'означать': [[]],\n",
       " 'определяться': [[('роль', 'NOUN', 'ablt'),\n",
       "   ('в', 'PREP'),\n",
       "   ('раскрытие', 'NOUN', 'loct'),\n",
       "   ('к', 'PREP'),\n",
       "   ('геометрия', 'NOUN', 'gent')]],\n",
       " 'основываться': [[('на', 'PREP'), ('воззрение', 'NOUN', 'loct')]],\n",
       " 'осуществлять': [],\n",
       " 'позволять': [[], []],\n",
       " 'поражать': [],\n",
       " 'приводить': [],\n",
       " 'продвигать': [[('наука', 'NOUN', 'nomn')]],\n",
       " 'располагать': [],\n",
       " 'решать': [[('возникновение', 'NOUN', 'nomn'),\n",
       "   ('развитие', 'NOUN', 'accs'),\n",
       "   ('необходимость', 'NOUN', 'ablt')],\n",
       "  []],\n",
       " 'свидетельствовать': [[('памятник', 'NOUN', 'nomn'), ('о', 'PREP')]],\n",
       " 'связывать': [],\n",
       " 'служить': [[('метод', 'NOUN', 'nomn'),\n",
       "   ('из', 'PREP'),\n",
       "   ('инструмент', 'NOUN', 'gent')],\n",
       "  [('храм', 'NOUN', 'nomn'), ('подтверждение', 'NOUN', 'ablt')],\n",
       "  [('ключ', 'NOUN', 'ablt'), ('к', 'PREP'), ('открытие', 'NOUN', 'datv')],\n",
       "  [('предмет', 'NOUN', 'ablt')],\n",
       "  [('пример', 'NOUN', 'ablt')]],\n",
       " 'совершать': [],\n",
       " 'создавать': [],\n",
       " 'состоять': [[('заслуга', 'NOUN', 'nomn'),\n",
       "   ('в', 'PREP'),\n",
       "   ('систематизация', 'NOUN', 'gent'),\n",
       "   ('придание', 'NOUN', 'loct'),\n",
       "   ('изложение', 'NOUN', 'datv')],\n",
       "  []],\n",
       " 'сохраняться': [],\n",
       " 'способствовать': [[('формирование', 'NOUN', 'datv'),\n",
       "   ('о', 'PREP'),\n",
       "   ('пространство', 'NOUN', 'loct')],\n",
       "  [('освоение', 'NOUN', 'datv')],\n",
       "  []],\n",
       " 'становиться': [[('благодаря', 'PREP'),\n",
       "   ('греция', 'NOUN', 'gent'),\n",
       "   ('теория', 'NOUN', 'ablt')]],\n",
       " 'убеждать': [[('в', 'PREP')]]}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_valences(contexts)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_computed_valences(contexts):\n",
    "    model = get_valences(contexts)\n",
    "    computed_models = {}\n",
    "    for key, value in model.items():\n",
    "        verb_model = {}\n",
    "        for sent in range(len(value)):\n",
    "            for idx, word in enumerate(value[sent]):\n",
    "                if word[1] == 'PREP':\n",
    "                    if idx < (len(value[sent]) - 1):\n",
    "                        if value[sent][idx+1][1] == 'NOUN':\n",
    "                            n_info = value[sent][idx+1]\n",
    "                            if word[0] not in verb_model:\n",
    "                                verb_model[word[0] + ' + ' + n_info[2]] = {n_info[0] : 1}\n",
    "                            else:\n",
    "                                if n_info[0] in verb_model[word[0] + ' + ' + n_info[2]]:\n",
    "                                    verb_model[word[0] + ' + ' + n_info[2]][n_info[0]] += 1\n",
    "                                else:\n",
    "                                    verb_model[word[0] + ' + ' + n_info[2]][n_info[0]] = 1\n",
    "                if word[1] == 'NOUN':\n",
    "                    if idx != 0:\n",
    "                        if value[sent][idx-1][1] != 'PREP':\n",
    "                            if word[2] == 'nomn':\n",
    "                                if 'подлежащее, nomn' not in verb_model:\n",
    "                                    verb_model['подлежащее, nomn'] = {n_info[0] : 1}\n",
    "                                else:\n",
    "                                    if n_info[0] in verb_model['подлежащее, nomn']:\n",
    "                                        verb_model['подлежащее, nomn'] += 1\n",
    "                                    else:\n",
    "                                        verb_model['подлежащее, nomn'] = 1\n",
    "                            else:\n",
    "                                v_gr = str(pmm.tag(key)[0])\n",
    "                                is_tran = v_gr.split()[0].split(',')[2]\n",
    "                                if is_tran == 'tran':\n",
    "                                    if word[2] == 'accs':\n",
    "                                        if 'прямое дополнение, acc' not in verb_model:\n",
    "                                            verb_model['прямое дополнение, acc'] = {n_info[0] : 1}\n",
    "                                        else:\n",
    "                                            if n_info[0] in verb_model['прямое дополнение, acc']:\n",
    "                                                verb_model['прямое дополнение, acc'] += 1\n",
    "                                            else:\n",
    "                                                verb_model['прямое дополнение, acc'] = 1\n",
    "                                    else:\n",
    "                                        title = 'непрямое дополнение, ' + word[2]\n",
    "                                        if title not in verb_model:\n",
    "                                            verb_model[title] = {n_info[0] : 1}\n",
    "                                        else:\n",
    "                                            if n_info[0] in verb_model[title]:\n",
    "                                                verb_model[title] += 1\n",
    "                                            else:\n",
    "                                                verb_model[title] = 1\n",
    "        computed_models[key] = verb_model\n",
    "        \n",
    "    return(computed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_models = getting_computed_valences(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json('test computed models.json', computed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'базироваться': {'на + nomn': {'мироздание': 1}},\n",
       " 'владеть': {},\n",
       " 'вносить': {},\n",
       " 'вселять': {},\n",
       " 'выражаться': {'подлежащее, nomn': {'мироздание': 1}},\n",
       " 'вычислять': {},\n",
       " 'доходить': {},\n",
       " 'измерять': {},\n",
       " 'изображать': {},\n",
       " 'изучаться': {'в + gent': {'планиметрия': 1}, 'в + loct': {'основное': 1}},\n",
       " 'исчерпывать': {},\n",
       " 'лежать': {'в + gent': {'плоскость': 1}},\n",
       " 'накоплять': {},\n",
       " 'начинать': {},\n",
       " 'обладать': {},\n",
       " 'обусловливать': {},\n",
       " 'означать': {},\n",
       " 'определяться': {'в + loct': {'раскрытие': 1}, 'к + gent': {'геометрия': 1}},\n",
       " 'основываться': {'на + loct': {'воззрение': 1}},\n",
       " 'осуществлять': {},\n",
       " 'позволять': {},\n",
       " 'поражать': {},\n",
       " 'приводить': {},\n",
       " 'продвигать': {},\n",
       " 'располагать': {},\n",
       " 'решать': {'непрямое дополнение, ablt': {'геометрия': 1},\n",
       "  'прямое дополнение, acc': {'геометрия': 1}},\n",
       " 'свидетельствовать': {},\n",
       " 'связывать': {},\n",
       " 'служить': {'из + gent': {'инструмент': 1},\n",
       "  'к + datv': {'открытие': 1},\n",
       "  'непрямое дополнение, ablt': {'инструмент': 1}},\n",
       " 'совершать': {},\n",
       " 'создавать': {},\n",
       " 'состоять': {'в + gent': {'систематизация': 1}},\n",
       " 'сохраняться': {},\n",
       " 'способствовать': {'о + loct': {'пространство': 1}},\n",
       " 'становиться': {'благодаря + gent': {'греция': 1}},\n",
       " 'убеждать': {}}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_valences(computed_models):\n",
    "    for verb, parse in computed_models.items():\n",
    "        if parse:\n",
    "            with open('test valences/Модель управления глагола ' + verb + '.txt', 'a', encoding = 'utf-8') as v:\n",
    "                v.write('Модель управления глагола ' + verb + ':\\n')\n",
    "                for obj, exmpls in parse.items():\n",
    "                    v.write(obj + ': ' )\n",
    "                    for exmpl, value in exmpls.items():\n",
    "                        v.write(exmpl + ' = ' + str(value) + '; ')\n",
    "                    v.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_valences(computed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
